{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lambda-DynamoDB based API with AWS lambda powertools This is a example project for an AWS serverless API deployed with CDK and written in python. It uses: lambdas for integration with http api gateway, see api and refs/api lambda layers to manage dependencies, see layers including AWS Powertools library as a micro-framework dynamodb for persistence, see refs/models dynamodb streams to generate api triggered change events, see refs/events cdk to deploy the infrastructure, see deploy standard mkdocs static docs generator, see docs mkdocstrings plugin to generate documentation from the source files. Configuration You will need to have a .env file created in the root of this project to properly configure and deploy the stacks, see config for details on the variables exported and used. Setup and deployment instructions You can use the included makefile to quickly setup the environment, test and deploy the environment, see deploy for instructions on setup and deployment and the makefile for details the make targets available.","title":"Home"},{"location":"#lambda-dynamodb-based-api-with-aws-lambda-powertools","text":"This is a example project for an AWS serverless API deployed with CDK and written in python. It uses: lambdas for integration with http api gateway, see api and refs/api lambda layers to manage dependencies, see layers including AWS Powertools library as a micro-framework dynamodb for persistence, see refs/models dynamodb streams to generate api triggered change events, see refs/events cdk to deploy the infrastructure, see deploy standard mkdocs static docs generator, see docs mkdocstrings plugin to generate documentation from the source files.","title":"Lambda-DynamoDB based API with AWS lambda powertools"},{"location":"#configuration","text":"You will need to have a .env file created in the root of this project to properly configure and deploy the stacks, see config for details on the variables exported and used.","title":"Configuration"},{"location":"#setup-and-deployment-instructions","text":"You can use the included makefile to quickly setup the environment, test and deploy the environment, see deploy for instructions on setup and deployment and the makefile for details the make targets available.","title":"Setup and deployment instructions"},{"location":"api/","text":"","title":"API"},{"location":"deploy/","text":"Infrastructure as Code with CDK This project uses cdk to deploy the full service stack. The CDK app app.py and cdk stacks infra_stack.py included under infra are used together to create the services in aws for this api microservice. See Reference/CDK Infra for details on the constructs created and deployed. Makefile setup and deployment instructions You can use the included makefile to quickly setup the environment, test and deploy the environment. You will need to have a .env file created in the root of this project, see .env for details. Additionally, you will need to have the make system installed on your workstation. To build the virtual environment and install the dependencies as well as build the layer zip file for deployment: $ make deps After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate At this point you can now test the code and synthesize the CloudFormation template for this code. $ make test And finally, manually deploy $ make deploy For documentation of the full set of makefile targets included see: Makefile . Standard CDK setup Instructions To use the standard CDK setup and deployment methods, first manually create a virtualenv on MacOS and Linux: $ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate If you are a Windows platform, you would activate the virtualenv like this: % .venv\\Scripts\\activate.bat Once the virtualenv is activated, you can install the required dependencies. $ pip install -r requirements.txt At this point you can now synthesize the CloudFormation template for this code. $ cdk synth To add additional dependencies, for example other CDK libraries, just add them to your setup.py file and rerun the pip install -r requirements.txt command. Useful commands cdk ls list all stacks in the app cdk synth emits the synthesized CloudFormation template cdk deploy deploy this stack to your default AWS account/region cdk diff compare deployed stack with current state cdk docs open CDK documentation Enjoy!","title":"Deploy"},{"location":"deploy/#infrastructure-as-code-with-cdk","text":"This project uses cdk to deploy the full service stack. The CDK app app.py and cdk stacks infra_stack.py included under infra are used together to create the services in aws for this api microservice. See Reference/CDK Infra for details on the constructs created and deployed.","title":"Infrastructure as Code with CDK"},{"location":"deploy/#makefile-setup-and-deployment-instructions","text":"You can use the included makefile to quickly setup the environment, test and deploy the environment. You will need to have a .env file created in the root of this project, see .env for details. Additionally, you will need to have the make system installed on your workstation. To build the virtual environment and install the dependencies as well as build the layer zip file for deployment: $ make deps After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate At this point you can now test the code and synthesize the CloudFormation template for this code. $ make test And finally, manually deploy $ make deploy For documentation of the full set of makefile targets included see: Makefile .","title":"Makefile setup and deployment instructions"},{"location":"deploy/#standard-cdk-setup-instructions","text":"To use the standard CDK setup and deployment methods, first manually create a virtualenv on MacOS and Linux: $ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate If you are a Windows platform, you would activate the virtualenv like this: % .venv\\Scripts\\activate.bat Once the virtualenv is activated, you can install the required dependencies. $ pip install -r requirements.txt At this point you can now synthesize the CloudFormation template for this code. $ cdk synth To add additional dependencies, for example other CDK libraries, just add them to your setup.py file and rerun the pip install -r requirements.txt command.","title":"Standard CDK setup Instructions"},{"location":"deploy/#useful-commands","text":"cdk ls list all stacks in the app cdk synth emits the synthesized CloudFormation template cdk deploy deploy this stack to your default AWS account/region cdk diff compare deployed stack with current state cdk docs open CDK documentation Enjoy!","title":"Useful commands"},{"location":"docs/","text":"Documentation This project uses mkdocs to generate the core documentation you are reading, and mkdocstrings to generate the source reference documentation from docstrings. Generation and Deployment We use github actions to generate the static html and github pages to publish and host them. See the workflow for how deployment works.","title":"Docs"},{"location":"docs/#documentation","text":"This project uses mkdocs to generate the core documentation you are reading, and mkdocstrings to generate the source reference documentation from docstrings.","title":"Documentation"},{"location":"docs/#generation-and-deployment","text":"We use github actions to generate the static html and github pages to publish and host them. See the workflow for how deployment works.","title":"Generation and Deployment"},{"location":"dotenv/","text":".env reference This project uses a .env file to configure both the infrastructure stacks and the makefile targets, it requires the following values to be defined: LAYER_VERSION: semantic versioning suffix for layer archive name, used by make and by cdk LAYER_NAME: prefix on the layer archive file (zip) used by make and cdk STACK_NAME: used by cdk to name the stack and stack resources LOG_LEVEL: sets logging level in the lambdas DEPLOY_ACCOUNT: AWS account id to deploy in DEPLOY_REGION: AWS region to deploy to","title":"Config"},{"location":"dotenv/#env-reference","text":"This project uses a .env file to configure both the infrastructure stacks and the makefile targets, it requires the following values to be defined: LAYER_VERSION: semantic versioning suffix for layer archive name, used by make and by cdk LAYER_NAME: prefix on the layer archive file (zip) used by make and cdk STACK_NAME: used by cdk to name the stack and stack resources LOG_LEVEL: sets logging level in the lambdas DEPLOY_ACCOUNT: AWS account id to deploy in DEPLOY_REGION: AWS region to deploy to","title":".env reference"},{"location":"layers/","text":"Layers This project uses lambda layers to help deploy and manage dependencies for the lambdas. The main dependency used is the AWS lambda powertools library which provides extremely useful classes, decorators and methods for: simplifying the use of metrics, logging and x-ray tracing validating and parsing common AWS service event/request types including pydantic data models for custom business logic, parsing and validation Configuring and Packaging This project uses a makefile to simplify the installation of the required libraries and the archive (zip) creation for deploying the layer to AWS. A requirements.txt file is included and versioned in the layers folder to manage those dependencies separately from the development dependencies in the root requirements file. The environment variables required for layer archive creation are: LAYER_VERSION: semantic versioning suffix for layer archive name, used by make and by cdk LAYER_NAME: prefix on the layer archive file (zip) used by make and cdk","title":"Layer"},{"location":"layers/#layers","text":"This project uses lambda layers to help deploy and manage dependencies for the lambdas. The main dependency used is the AWS lambda powertools library which provides extremely useful classes, decorators and methods for: simplifying the use of metrics, logging and x-ray tracing validating and parsing common AWS service event/request types including pydantic data models for custom business logic, parsing and validation","title":"Layers"},{"location":"layers/#configuring-and-packaging","text":"This project uses a makefile to simplify the installation of the required libraries and the archive (zip) creation for deploying the layer to AWS. A requirements.txt file is included and versioned in the layers folder to manage those dependencies separately from the development dependencies in the root requirements file. The environment variables required for layer archive creation are: LAYER_VERSION: semantic versioning suffix for layer archive name, used by make and by cdk LAYER_NAME: prefix on the layer archive file (zip) used by make and cdk","title":"Configuring and Packaging"},{"location":"makefile/","text":"Makefile This makefile is meant to ease the use of managing dependencies and building the layer archive (zip) for deployment. Running make with no target will give you a list of the available targets and their purpose. You must also have two environment variables either defined and exported or written in a .env file for layer processing to execute correctly. See the Config doc for more details. Targets in the makefile list (or just make) -- list all the targets in this file with a description deps -- rebuild both venv and local layer clean -- rm cdk, layer, and venv venv -- rebuild venv from scratch layer -- build clean layer zip test -- runs tests on lambda src and cdk stacks test-src -- runs pytest and cdk synth test-cdk -- executes cdk synth build-venv -- create venv if not there pip-venv -- install reqs into venv pip-layer -- install reqs into local dir for layer zip-layer -- zip up the layer packages for asset deployment clean-cdk -- clean the cdk.out dir clean-layer -- clean the layer libs clean-venv -- clean the venv dir","title":"Makefile"},{"location":"makefile/#makefile","text":"This makefile is meant to ease the use of managing dependencies and building the layer archive (zip) for deployment. Running make with no target will give you a list of the available targets and their purpose. You must also have two environment variables either defined and exported or written in a .env file for layer processing to execute correctly. See the Config doc for more details.","title":"Makefile"},{"location":"makefile/#targets-in-the-makefile","text":"list (or just make) -- list all the targets in this file with a description deps -- rebuild both venv and local layer clean -- rm cdk, layer, and venv venv -- rebuild venv from scratch layer -- build clean layer zip test -- runs tests on lambda src and cdk stacks test-src -- runs pytest and cdk synth test-cdk -- executes cdk synth build-venv -- create venv if not there pip-venv -- install reqs into venv pip-layer -- install reqs into local dir for layer zip-layer -- zip up the layer packages for asset deployment clean-cdk -- clean the cdk.out dir clean-layer -- clean the layer libs clean-venv -- clean the venv dir","title":"Targets in the makefile"},{"location":"refs/api/","text":"","title":"Api"},{"location":"refs/api/#api","text":"","title":"api"},{"location":"refs/events/","text":"dynamo_stream_handler ( in_event , context ) This method when registered as a lambda will handle incoming dynamodb stream events from a connected dynamodb. It transforms those events using the ModelChangeEvent model and puts them out to eventbridge as a custom event. This way business logic for change events are encapsulated in the models package. Parameters: Name Type Description Default in_event Dict[str, Any] the incoming event required context LambdaContext the context required Returns: Type Description Dict[str, Any] Dict[str, Any]: Response converted to dict, not typically consumed Source code in src/events.py @logger . inject_lambda_context ( log_event = True ) def dynamo_stream_handler ( in_event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: \"\"\" This method when registered as a lambda will handle incoming dynamodb stream events from a connected dynamodb. It transforms those events using the ModelChangeEvent model and puts them out to eventbridge as a custom event. This way business logic for change events are encapsulated in the models package. Args: in_event (Dict[str, Any]): the incoming event context (LambdaContext): the context Returns: Dict[str, Any]: Response converted to dict, not typically consumed \"\"\" global eventbridge if eventbridge is None : eventbridge = boto3 . client ( \"events\" ) response = Response () ddb_model : DynamoDBStreamModel = parse ( model = DynamoDBStreamModel , event = in_event ) logger . info ( f \"DynamodbStreamEvent Received: { ddb_model } \" ) for record in ddb_model . Records : out_event : ModelChangeEvent = None if record . eventName == 'INSERT' : out_event = ModelChangeEvent . from_dynamodb_record ( \"CREATE\" , record ) elif record . eventName == 'MODIFY' : out_event = ModelChangeEvent . from_dynamodb_record ( \"UPDATE\" , record ) elif record . eventName == 'REMOVE' : out_event = ModelChangeEvent . from_dynamodb_record ( \"DELETE\" , record ) try : logger . info ( out_event ) result = out_event . send ( eventbridge ) except ClientError as ce : response . add_boto_error ( ce ) logger . error ( ce ) except AttributeError as ae : me = ModelError ( title = \"Empty ModelChangeEvent Object\" , status = 500 , detail = str ( ae ) ) response . add_model_error ( me ) logger . error ( ae ) else : logger . info ( result ) return response . dump ()","title":"Events"},{"location":"refs/events/#events","text":"","title":"events"},{"location":"refs/events/#events.dynamo_stream_handler","text":"This method when registered as a lambda will handle incoming dynamodb stream events from a connected dynamodb. It transforms those events using the ModelChangeEvent model and puts them out to eventbridge as a custom event. This way business logic for change events are encapsulated in the models package. Parameters: Name Type Description Default in_event Dict[str, Any] the incoming event required context LambdaContext the context required Returns: Type Description Dict[str, Any] Dict[str, Any]: Response converted to dict, not typically consumed Source code in src/events.py @logger . inject_lambda_context ( log_event = True ) def dynamo_stream_handler ( in_event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: \"\"\" This method when registered as a lambda will handle incoming dynamodb stream events from a connected dynamodb. It transforms those events using the ModelChangeEvent model and puts them out to eventbridge as a custom event. This way business logic for change events are encapsulated in the models package. Args: in_event (Dict[str, Any]): the incoming event context (LambdaContext): the context Returns: Dict[str, Any]: Response converted to dict, not typically consumed \"\"\" global eventbridge if eventbridge is None : eventbridge = boto3 . client ( \"events\" ) response = Response () ddb_model : DynamoDBStreamModel = parse ( model = DynamoDBStreamModel , event = in_event ) logger . info ( f \"DynamodbStreamEvent Received: { ddb_model } \" ) for record in ddb_model . Records : out_event : ModelChangeEvent = None if record . eventName == 'INSERT' : out_event = ModelChangeEvent . from_dynamodb_record ( \"CREATE\" , record ) elif record . eventName == 'MODIFY' : out_event = ModelChangeEvent . from_dynamodb_record ( \"UPDATE\" , record ) elif record . eventName == 'REMOVE' : out_event = ModelChangeEvent . from_dynamodb_record ( \"DELETE\" , record ) try : logger . info ( out_event ) result = out_event . send ( eventbridge ) except ClientError as ce : response . add_boto_error ( ce ) logger . error ( ce ) except AttributeError as ae : me = ModelError ( title = \"Empty ModelChangeEvent Object\" , status = 500 , detail = str ( ae ) ) response . add_model_error ( me ) logger . error ( ae ) else : logger . info ( result ) return response . dump ()","title":"dynamo_stream_handler()"},{"location":"refs/infra/","text":"InfraStack InfraStack is the infrastructure deployment stack for the api. The basic architecture is a simple rest api handled by a lambda which persists changes to a dynamodb table. Any changes in the dynamodb table generate a stream event which is handled by the event handler lambda. The event handler lambda converts a dynamodb stream event into a custom event and puts that event onto the eventbridge default bus. Dependencies for the lambdas are handled in the lambda_layer. A cloudwatch log-group is created to log any events put to eventbridge by the event lambda. Attributes: Name Type Description ddb_table DyanmoDB dynamodb table with a primary key fn_layer Lambda Layer a lambda layer with the import requirements for the lambda api_lambda Lambda a lambda to handle api requests evt_lambda Lambda a lambda to handle dynamodb stream events http_api Http apigateway v2 a http api gateway v2 http_api.route HTTP proxy a route to handle any method and all requests to /api event_log_group CloudwatchLogGroup captures events generated by evt_lambda","title":"CDK"},{"location":"refs/infra/#infra_stack","text":"","title":"infra_stack"},{"location":"refs/infra/#infra_stack.InfraStack","text":"InfraStack is the infrastructure deployment stack for the api. The basic architecture is a simple rest api handled by a lambda which persists changes to a dynamodb table. Any changes in the dynamodb table generate a stream event which is handled by the event handler lambda. The event handler lambda converts a dynamodb stream event into a custom event and puts that event onto the eventbridge default bus. Dependencies for the lambdas are handled in the lambda_layer. A cloudwatch log-group is created to log any events put to eventbridge by the event lambda. Attributes: Name Type Description ddb_table DyanmoDB dynamodb table with a primary key fn_layer Lambda Layer a lambda layer with the import requirements for the lambda api_lambda Lambda a lambda to handle api requests evt_lambda Lambda a lambda to handle dynamodb stream events http_api Http apigateway v2 a http api gateway v2 http_api.route HTTP proxy a route to handle any method and all requests to /api event_log_group CloudwatchLogGroup captures events generated by evt_lambda","title":"InfraStack"},{"location":"refs/models/","text":"Model pydantic-model This class is the model of our API domain object Attributes: Name Type Description guid str a generated uuid4 string name str name for this objec metadata dict dictionary for arbitrary data of the object from_dbstream_image ( db_json ) classmethod this factory method parses raw dyanamodb json in python dict form and generates a well formed model from it. Parameters: Name Type Description Default db_json Dict The dynamodb json format in a python dict: {key: { type: value}} required Returns: Type Description [type] [description] Source code in src/models.py @classmethod def from_dbstream_image ( cls , db_json : Dict ): \"\"\" this factory method parses raw dyanamodb json in python dict form and generates a well formed model from it. Args: db_json (Dict): The dynamodb json format in a python dict: {key: { type: value}} Returns: [type]: [description] \"\"\" data = json_util . loads ( db_json ) return cls ( ** data ) validate_guid ( g ) classmethod This fuction validates guid arg passed to constructor It will catch a ValueError on conversion to UUID, and generate a new uuid4 in its place. Parameters: Name Type Description Default g str str passed to constructor for guid required Returns: Type Description str str: returns a true string instance of a hex representation of a uuid4 Source code in src/models.py @validator ( 'guid' ) def validate_guid ( cls , g : str ) -> str : \"\"\" This fuction validates guid arg passed to constructor It will catch a ValueError on conversion to UUID, and generate a new uuid4 in its place. Args: g (str): str passed to constructor for guid Returns: str: returns a true string instance of a hex representation of a uuid4 \"\"\" try : vg = UUID ( g ) except ValueError as e : vg = uuid4 () return str ( vg ) ModelChangeEvent pydantic-model ModelChangeEvent class encapsulates the data structure and functionality for a Model change event generated by dynamodb streams. Using the send event a instance will be put to an eventbridge client. Attributes: Name Type Description Source str (str): [description] EventBusName Optional[str] (str): (Optional) if not specified event is sent to default Resources Optional[List[str]] (list[str]): (Optional) DetailType str (str): the identifier of the type of detail Detail ModelEventDetail (ModelEventDetail): the data passed from dynamodb streams dump ( self ) This creates a properly formatted response dictionary to return from a lambda handler call Source code in src/models.py def dump ( self ) -> Dict : \"\"\" This creates a properly formatted response dictionary to return from a lambda handler call \"\"\" me = self . dict ( exclude = { 'Detail' }, exclude_unset = True ) me [ 'Detail' ] = '' if self . Detail is not None : me [ 'Detail' ] = self . Detail . json () return me from_dynamodb_record ( event_type , record ) classmethod Factory classmethod to generate a well-formed ModelChangeEvent from an incoming dynamodb stream event. Parameters: Name Type Description Default event_type str Identifier for event type being transcribed required record DynamoDBStreamRecordModel incoming event from dynamodb stream required Returns: Type Description [ModelChangeEvent] Instance of ModelChangeEvent Source code in src/models.py @classmethod def from_dynamodb_record ( cls , event_type : str , record : DynamoDBStreamRecordModel ): \"\"\" Factory classmethod to generate a well-formed ModelChangeEvent from an incoming dynamodb stream event. Args: event_type (str): Identifier for event type being transcribed record (DynamoDBStreamRecordModel): incoming event from dynamodb stream Returns: [ModelChangeEvent]: Instance of ModelChangeEvent \"\"\" old_model = None new_model = None if record . dynamodb . OldImage is not None : old_model = Model . from_dbstream_image ( record . dynamodb . OldImage ) if record . dynamodb . NewImage is not None : new_model = Model . from_dbstream_image ( record . dynamodb . NewImage ) model_id = json_util . loads ( record . dynamodb . Keys )[ 'guid' ] evtd = ModelEventDetail ( event_type = event_type , old_model = old_model , new_model = new_model , model_id = model_id ) evt = cls ( Source = \"model.api.events\" , DetailType = f \"model.change. { event_type . lower () } \" , Detail = evtd ) return evt send ( self , eventbridge ) Send takes boto client connection to eventbridge and executes a put_event for the serialized version of this object Parameters: Name Type Description Default eventbridge <function client at 0x7fc67de11ca0> the provisioned boto3 client ('events') required Returns: Type Description result [dict] the boto3 response from the api call put_event Source code in src/models.py def send ( self , eventbridge : boto3 . client ): \"\"\"Send takes boto client connection to eventbridge and executes a put_event for the serialized version of this object Args: eventbridge (boto3.client): the provisioned boto3 client ('events') Returns: result [dict]: the boto3 response from the api call put_event \"\"\" result = eventbridge . put_events ( Entries = [ self . dump ()]) return result ModelError pydantic-model Class for IETF RFC error schema, we use this to return errors in the response Attributes: Name Type Description type str A URI identifier that categorizes the error title str A brief, human-readable message about the error status str The HTTP response code (optional) detail str A human-readable explanation of the error instance str A URI that identifies the specific occurrence of the error ModelEventDetail pydantic-model ModelEventDetail class is a data structure consumed by ModelChangeEvent. When serialized it comprises the detail field of an eventbridge event Attributes: Name Type Description model_id str the uuid for the model event_type str unique identifier for event old_model Optional[models.Model] (Model): (Optional) may be present if a change or deletion has occured new_model Optional[models.Model] (Model): (Optional) may be present if a change or creation has occured ModelStore ModelStore class encapsulates the persistence layer functions for the 'Model' in a dynamodb table Response pydantic-model Response encapsulates a full response payload from a lambda call for v2 of the payload format The body of the payload is a ResponseBody object Attributes: Name Type Description statusCode int http response code headers dict dictionary of http headers, defaults with 'Content-Type': 'application/json' isBase64Encoded bool (bool): default is false cookies list list http cookies body ResponseBody makes up the body of the response dump ( self ) This creates a properly formatted response dictionary to return from a lambda handler call Source code in src/models.py def dump ( self ) -> Dict : ''' This creates a properly formatted response dictionary to return from a lambda handler call ''' me = self . dict ( exclude = { 'body' }) me [ 'body' ] = '' if self . body is not None : me [ 'body' ] = self . body . json () return me ResponseBody pydantic-model Class for body of http response payload Includes a list of errors if any are generated and a list of models that have been affected Attributes: Name Type Description errors list of ModelErrors encounterd models list of models returned","title":"Models"},{"location":"refs/models/#models","text":"","title":"models"},{"location":"refs/models/#models.Model","text":"This class is the model of our API domain object Attributes: Name Type Description guid str a generated uuid4 string name str name for this objec metadata dict dictionary for arbitrary data of the object","title":"Model"},{"location":"refs/models/#models.Model.from_dbstream_image","text":"this factory method parses raw dyanamodb json in python dict form and generates a well formed model from it. Parameters: Name Type Description Default db_json Dict The dynamodb json format in a python dict: {key: { type: value}} required Returns: Type Description [type] [description] Source code in src/models.py @classmethod def from_dbstream_image ( cls , db_json : Dict ): \"\"\" this factory method parses raw dyanamodb json in python dict form and generates a well formed model from it. Args: db_json (Dict): The dynamodb json format in a python dict: {key: { type: value}} Returns: [type]: [description] \"\"\" data = json_util . loads ( db_json ) return cls ( ** data )","title":"from_dbstream_image()"},{"location":"refs/models/#models.Model.validate_guid","text":"This fuction validates guid arg passed to constructor It will catch a ValueError on conversion to UUID, and generate a new uuid4 in its place. Parameters: Name Type Description Default g str str passed to constructor for guid required Returns: Type Description str str: returns a true string instance of a hex representation of a uuid4 Source code in src/models.py @validator ( 'guid' ) def validate_guid ( cls , g : str ) -> str : \"\"\" This fuction validates guid arg passed to constructor It will catch a ValueError on conversion to UUID, and generate a new uuid4 in its place. Args: g (str): str passed to constructor for guid Returns: str: returns a true string instance of a hex representation of a uuid4 \"\"\" try : vg = UUID ( g ) except ValueError as e : vg = uuid4 () return str ( vg )","title":"validate_guid()"},{"location":"refs/models/#models.ModelChangeEvent","text":"ModelChangeEvent class encapsulates the data structure and functionality for a Model change event generated by dynamodb streams. Using the send event a instance will be put to an eventbridge client. Attributes: Name Type Description Source str (str): [description] EventBusName Optional[str] (str): (Optional) if not specified event is sent to default Resources Optional[List[str]] (list[str]): (Optional) DetailType str (str): the identifier of the type of detail Detail ModelEventDetail (ModelEventDetail): the data passed from dynamodb streams","title":"ModelChangeEvent"},{"location":"refs/models/#models.ModelChangeEvent.dump","text":"This creates a properly formatted response dictionary to return from a lambda handler call Source code in src/models.py def dump ( self ) -> Dict : \"\"\" This creates a properly formatted response dictionary to return from a lambda handler call \"\"\" me = self . dict ( exclude = { 'Detail' }, exclude_unset = True ) me [ 'Detail' ] = '' if self . Detail is not None : me [ 'Detail' ] = self . Detail . json () return me","title":"dump()"},{"location":"refs/models/#models.ModelChangeEvent.from_dynamodb_record","text":"Factory classmethod to generate a well-formed ModelChangeEvent from an incoming dynamodb stream event. Parameters: Name Type Description Default event_type str Identifier for event type being transcribed required record DynamoDBStreamRecordModel incoming event from dynamodb stream required Returns: Type Description [ModelChangeEvent] Instance of ModelChangeEvent Source code in src/models.py @classmethod def from_dynamodb_record ( cls , event_type : str , record : DynamoDBStreamRecordModel ): \"\"\" Factory classmethod to generate a well-formed ModelChangeEvent from an incoming dynamodb stream event. Args: event_type (str): Identifier for event type being transcribed record (DynamoDBStreamRecordModel): incoming event from dynamodb stream Returns: [ModelChangeEvent]: Instance of ModelChangeEvent \"\"\" old_model = None new_model = None if record . dynamodb . OldImage is not None : old_model = Model . from_dbstream_image ( record . dynamodb . OldImage ) if record . dynamodb . NewImage is not None : new_model = Model . from_dbstream_image ( record . dynamodb . NewImage ) model_id = json_util . loads ( record . dynamodb . Keys )[ 'guid' ] evtd = ModelEventDetail ( event_type = event_type , old_model = old_model , new_model = new_model , model_id = model_id ) evt = cls ( Source = \"model.api.events\" , DetailType = f \"model.change. { event_type . lower () } \" , Detail = evtd ) return evt","title":"from_dynamodb_record()"},{"location":"refs/models/#models.ModelChangeEvent.send","text":"Send takes boto client connection to eventbridge and executes a put_event for the serialized version of this object Parameters: Name Type Description Default eventbridge <function client at 0x7fc67de11ca0> the provisioned boto3 client ('events') required Returns: Type Description result [dict] the boto3 response from the api call put_event Source code in src/models.py def send ( self , eventbridge : boto3 . client ): \"\"\"Send takes boto client connection to eventbridge and executes a put_event for the serialized version of this object Args: eventbridge (boto3.client): the provisioned boto3 client ('events') Returns: result [dict]: the boto3 response from the api call put_event \"\"\" result = eventbridge . put_events ( Entries = [ self . dump ()]) return result","title":"send()"},{"location":"refs/models/#models.ModelError","text":"Class for IETF RFC error schema, we use this to return errors in the response Attributes: Name Type Description type str A URI identifier that categorizes the error title str A brief, human-readable message about the error status str The HTTP response code (optional) detail str A human-readable explanation of the error instance str A URI that identifies the specific occurrence of the error","title":"ModelError"},{"location":"refs/models/#models.ModelEventDetail","text":"ModelEventDetail class is a data structure consumed by ModelChangeEvent. When serialized it comprises the detail field of an eventbridge event Attributes: Name Type Description model_id str the uuid for the model event_type str unique identifier for event old_model Optional[models.Model] (Model): (Optional) may be present if a change or deletion has occured new_model Optional[models.Model] (Model): (Optional) may be present if a change or creation has occured","title":"ModelEventDetail"},{"location":"refs/models/#models.ModelStore","text":"ModelStore class encapsulates the persistence layer functions for the 'Model' in a dynamodb table","title":"ModelStore"},{"location":"refs/models/#models.Response","text":"Response encapsulates a full response payload from a lambda call for v2 of the payload format The body of the payload is a ResponseBody object Attributes: Name Type Description statusCode int http response code headers dict dictionary of http headers, defaults with 'Content-Type': 'application/json' isBase64Encoded bool (bool): default is false cookies list list http cookies body ResponseBody makes up the body of the response","title":"Response"},{"location":"refs/models/#models.Response.dump","text":"This creates a properly formatted response dictionary to return from a lambda handler call Source code in src/models.py def dump ( self ) -> Dict : ''' This creates a properly formatted response dictionary to return from a lambda handler call ''' me = self . dict ( exclude = { 'body' }) me [ 'body' ] = '' if self . body is not None : me [ 'body' ] = self . body . json () return me","title":"dump()"},{"location":"refs/models/#models.ResponseBody","text":"Class for body of http response payload Includes a list of errors if any are generated and a list of models that have been affected Attributes: Name Type Description errors list of ModelErrors encounterd models list of models returned","title":"ResponseBody"}]}